#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Llama-3.1 8B GPTQ INT4 성능 벤치 스크립트
"""

import os, time, math, psutil, torch
from datasets import load_dataset
from gptqmodel import GPTQModel, BACKEND
from transformers import logging as hf_logging
hf_logging.set_verbosity_error()

# ────────────────────────── 설정값 ──────────────────────────
MODEL_PATH   = "/home/eiclab/eiclab04/urp2025/gptq_inperson/Llama-3.1-8B-GPTQ-pplopt"
PROMPT       = "In a shocking finding, scientists discovered that "
NEW_TOKENS   = 128             # 추론 속도 테스트용
MAX_PPL_TOK  = 50_000            # PPL 평가에 사용할 최대 토큰 수
SEQ_LEN      = 2048              # 슬라이딩 윈도우
STRIDE       = 1024
BACKEND_SEL  = BACKEND.TRITON  # MARLIN 으로 바꾸면 마린 가속
BATCH_SIZE   = 1               #추론용 batch 크기
# ────────────────────────────────────────────────────────────


def gpu_mem_mb() -> float:
    return torch.cuda.memory_allocated() / 2**20

def build_token_ids(tokenizer, text: str, max_tokens: int) -> torch.Tensor:
    """긴 텍스트를 4 000자 단위로 잘라 토큰화 후 이어붙임 → 메모리·속도 최적"""
    ids = []
    for i in range(0, len(text), 4000):
        chunk = text[i:i+4000]
        ids.extend(tokenizer(
            chunk, add_special_tokens=False, truncation=False
        ).input_ids)
        if len(ids) >= max_tokens:
            break
    ids = ids[:max_tokens]
    return torch.tensor([ids])

def perplexity(model, tokenizer) -> float:
    ds   = load_dataset("wikitext", "wikitext-2-raw-v1", split="validation")
    text = "\n\n".join(ds["text"])
    ids  = build_token_ids(tokenizer, text, MAX_PPL_TOK).to("cuda")

    nll, ntok = 0.0, 0
    model.eval()
    with torch.no_grad():
        for i in range(0, ids.size(1) - 1, STRIDE):
            end  = min(i + SEQ_LEN, ids.size(1) - 1)
            inp  = ids[:, i:end]
            tgt  = ids[:, i+1:end+1]
            logits = model(input_ids=inp).logits
            logp   = torch.log_softmax(logits, -1)
            nll   += -(logp.gather(-1, tgt.unsqueeze(-1))
                             .squeeze(-1)).sum().item()
            ntok  += tgt.numel()
    return math.exp(nll / ntok)

def main():
    os.environ["CUDA_DEVICE_ORDER"]   = "PCI_BUS_ID"
    os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "max_split_size_mb:64")

    proc = psutil.Process()
    start_rss = proc.memory_info().rss / 2**20

    # ── 모델 로드 ──────────────────────────────────────────
    t0 = time.time()
    model = GPTQModel.load(MODEL_PATH, backend=BACKEND_SEL, device="cuda")
    load_sec  = time.time() - t0
    load_gpu  = gpu_mem_mb()
    load_rss  = proc.memory_info().rss / 2**20 - start_rss
    tok       = model.tokenizer

    # ── 추론 속도 측정 ────────────────────────────────────
    single_ids = tok(PROMPT, return_tensors="pt")
    ids = {k: v.repeat(BATCH_SIZE, 1).to("cuda") for k, v in single_ids.items()}
    torch.cuda.reset_peak_memory_stats()

    t0 = time.time()
    _  = model.generate(**ids, max_new_tokens=NEW_TOKENS)
    elapsed   = time.time() - t0
    tok_per_s = NEW_TOKENS * BATCH_SIZE / elapsed
    peak_gpu  = torch.cuda.max_memory_allocated() / 2**20

    # ── PPL 계산 ─────────────────────────────────────────
    ppl = perplexity(model, tok)

    # ── 결과 출력 ────────────────────────────────────────
    print("\n=== Llama-3.1 8B GPTQ Bench  ({}) ===".format(BACKEND_SEL.name))
    print(f"Load time         : {load_sec:6.1f}  s")
    print(f"Load GPU memory   : {load_gpu:6.1f} MiB")
    print(f"Load CPU RSS      : {load_rss:6.1f} MiB")
    print(f"Peak GPU memory   : {peak_gpu:6.1f} MiB")
    print(f"Inference batch   :      {BATCH_SIZE}")
    print(f"Inference speed   : {tok_per_s:6.1f} tokens/s "
      f"({NEW_TOKENS} each)")
    print(f"Wikitext-2 PPL    : {ppl:6.2f}")

if __name__ == "__main__":
    torch.cuda.empty_cache()
    main()
