{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9194b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gunwoong/awq-marlin/lib/python3.11/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ Quantizing version=Marlin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4923a4d958514cfa9a1de2a5717532a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "AWQ: 100%|██████████| 32/32 [12:27<00:00, 23.37s/it]\n",
      "Packing: 100%|██████████| 32/32 [00:00<00:00, 50705.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved to /home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-Marlin-INT4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1) torch shims\n",
    "if not hasattr(torch, \"get_default_device\"):\n",
    "    torch.get_default_device = lambda: torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if not hasattr(torch, \"xpu\"):\n",
    "    class _XPU:\n",
    "        @staticmethod\n",
    "        def is_available(): return False\n",
    "    torch.xpu = _XPU()\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path  = \"/home/gunwoong/URP2025-1/llama3-8b\"\n",
    "tokenizer   = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "common_cfg  = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4}\n",
    "\n",
    "versions = [\n",
    "    (\"GEMM\",\"/home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-GEMM-INT4\"),\n",
    "    (\"Marlin\",\"/home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-Marlin-INT4\")\n",
    "]\n",
    "\n",
    "for version, out_path in versions:\n",
    "    print(f\"\\n→ Quantizing version={version}\")\n",
    "    cfg = {**common_cfg, \"version\": version}\n",
    "    if version == \"Marlin\":\n",
    "        # Marlin은 zero-point 없이\n",
    "        cfg[\"zero_point\"] = False\n",
    "\n",
    "    # 1) load onto GPU\n",
    "    model = AutoAWQForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_cache=False,\n",
    "    )\n",
    "\n",
    "    # 2) quantize (+ verbose & fewer samples if too 느림)\n",
    "    model.quantize(\n",
    "        tokenizer,\n",
    "        quant_config=cfg,\n",
    "\n",
    "    )\n",
    "\n",
    "    # 3) pack into GPU memory\n",
    "    model.pack()\n",
    "\n",
    "    # 4) save\n",
    "    model.save_quantized(out_path, safetensors=True)\n",
    "    tokenizer.save_pretrained(out_path)\n",
    "    print(f\"  Saved to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301bcc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available:        True\n",
      "CUDA version (PyTorch): 12.6\n",
      "GPU count:             4\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 3090\n",
      "  Total memory: 23.6 GiB\n",
      "  Multi-Processor Count: 82\n",
      "  Compute Capability: 8.6\n",
      "\n",
      "GPU 1: NVIDIA GeForce RTX 3090\n",
      "  Total memory: 23.6 GiB\n",
      "  Multi-Processor Count: 82\n",
      "  Compute Capability: 8.6\n",
      "\n",
      "GPU 2: NVIDIA GeForce RTX 3090\n",
      "  Total memory: 23.6 GiB\n",
      "  Multi-Processor Count: 82\n",
      "  Compute Capability: 8.6\n",
      "\n",
      "GPU 3: NVIDIA GeForce RTX 3090\n",
      "  Total memory: 23.6 GiB\n",
      "  Multi-Processor Count: 82\n",
      "  Compute Capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:       \", torch.cuda.is_available())\n",
    "print(\"CUDA version (PyTorch):\", torch.version.cuda)\n",
    "print(\"GPU count:            \", torch.cuda.device_count())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"\\nGPU {i}: {props.name}\")\n",
    "    print(f\"  Total memory: {props.total_memory/1024**3:.1f} GiB\")\n",
    "    print(f\"  Multi-Processor Count: {props.multi_processor_count}\")\n",
    "    print(f\"  Compute Capability: {props.major}.{props.minor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652b5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (289076 > 131072). Running this sequence through the model will result in indexing errors\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "/home/gunwoong/awq-marlin/lib/python3.11/site-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['version', 'fuse_max_seq_len', 'exllama_config', 'modules_to_fuse', 'do_fuse']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "/home/gunwoong/URP2025-1/AutoAWQ/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f25f87dcbf47dba172274b7e879139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak PT alloc : 5474.0 MiB\n",
      "=== Meta-Llama-3.1-8B 8B INT4 Bench ===\n",
      "Load time           :  12.9 s\n",
      "Load GPU memory     :  5462.5 MiB  (PyTorch)\n",
      "Load GPU memory     :  5751.0 MiB  (nvidia-smi)\n",
      "Load CPU RSS        :  1056.6 MiB\n",
      "Inference peak GPU  :  9458.0 MiB\n",
      "Inference speed     :  3608.5 tokens/s (+2048)\n",
      "Wikitext-2 PPL      :    6.64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AwqConfig,\n",
    ")\n",
    "\n",
    "# ── Configuration ─────────────────────────────────────────────────────────────\n",
    "MODEL_NAME     = \"Meta-Llama-3.1-8B\"\n",
    "TOKENIZER_PATH = \"/home/gunwoong/URP2025-1/llama3-8b\"\n",
    "QUANT_PATH     = \"/home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-GEMM-INT4\"\n",
    "SEQLEN         = 2048\n",
    "DEVICE         = torch.device(\"cuda:1\")\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def fmt_mib(x: int) -> float:\n",
    "    return x / (1024**2)\n",
    "\n",
    "def get_driver_gpu_used(gpu_index: int = 0) -> float:\n",
    "    out = subprocess.check_output([\n",
    "        \"nvidia-smi\",\n",
    "        \"--query-gpu=memory.used\",\n",
    "        \"--format=csv,noheader,nounits\",\n",
    "        \"-i\", str(gpu_index)\n",
    "    ])\n",
    "    return max(float(x) for x in out.decode().splitlines() if x.strip())\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_ppl_and_speed(model, tokens, seqlen, device):\n",
    "    total_nll, total_toks = 0.0, 0\n",
    "    # make sure all kernels are done before timing\n",
    "    torch.cuda.synchronize(DEVICE)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in range(tokens.size(1) // seqlen):\n",
    "        chunk = tokens[:, i*seqlen:(i+1)*seqlen].to(DEVICE)\n",
    "        out   = model(chunk, labels=chunk)\n",
    "        total_nll += out.loss.item() * chunk.numel()\n",
    "        total_toks += chunk.numel()\n",
    "\n",
    "    # wait for last kernels\n",
    "    torch.cuda.synchronize(DEVICE)\n",
    "    t1 = time.time()\n",
    "    peak_inference = torch.cuda.max_memory_reserved(device)\n",
    "    ppl       = torch.exp(torch.tensor(total_nll/total_toks)).item()\n",
    "    tok_per_s = total_toks / (t1 - t0)\n",
    "    return ppl, tok_per_s, peak_inference\n",
    "\n",
    "def main():\n",
    "    # 1) Prepare tokens\n",
    "    ds       = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    tokenizer= AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)\n",
    "    big_txt  = \"\\n\\n\".join(ds[\"text\"])\n",
    "    ids      = tokenizer.encode(big_txt, add_special_tokens=False)\n",
    "    tokens   = torch.tensor([ids], dtype=torch.long)\n",
    "\n",
    "    # 2) Load & measure alloc stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    t_start = time.time()\n",
    "\n",
    "    awq_cfg = AwqConfig(bits=4, do_fuse=False, fuse_max_seq_len=SEQLEN)\n",
    "    model   = AutoModelForCausalLM.from_pretrained(\n",
    "        QUANT_PATH,\n",
    "        quantization_config=awq_cfg,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map={ \"\": DEVICE.index },\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    torch.cuda.synchronize(DEVICE)\n",
    "    peak = torch.cuda.max_memory_reserved(DEVICE)\n",
    "    print(f\"Peak PT alloc : {peak/1024**2:.1f} MiB\")\n",
    "    load_time    = time.time() - t_start\n",
    "    \n",
    "    # PyTorch allocator usage (optional)\n",
    "    gpu_used_tensor = fmt_mib(torch.cuda.memory_allocated(DEVICE))\n",
    "\n",
    "    # Real driver-reported peak (≈nvidia-smi)\n",
    "    gpu_used_driver = get_driver_gpu_used(1)\n",
    "\n",
    "    # CPU RSS\n",
    "    cpu_rss = fmt_mib(psutil.Process(os.getpid()).memory_info().rss)\n",
    "\n",
    "    # 3) Eval PPL + speed\n",
    "    ppl, speed, inf_peak = eval_ppl_and_speed(model, tokens, SEQLEN, DEVICE)\n",
    "\n",
    "    # 4) Print bench\n",
    "    print(f\"=== {MODEL_NAME} 8B INT4 Bench ===\")\n",
    "    print(f\"Load time           : {load_time:5.1f} s\")\n",
    "    print(f\"Load GPU memory     : {gpu_used_tensor:7.1f} MiB  (PyTorch)\")\n",
    "    print(f\"Load GPU memory     : {gpu_used_driver:7.1f} MiB  (nvidia-smi)\")  # ← your 9793 MiB\n",
    "    print(f\"Load CPU RSS        : {cpu_rss:7.1f} MiB\")\n",
    "    print(f\"Inference peak GPU  : {inf_peak/1024**2:7.1f} MiB\")\n",
    "    print(f\"Inference speed     : {speed:7.1f} tokens/s (+{SEQLEN})\")\n",
    "    print(f\"Wikitext-2 PPL      : {ppl:7.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a2578c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (289076 > 131072). Running this sequence through the model will result in indexing errors\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "/home/gunwoong/awq-marlin/lib/python3.11/site-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['version', 'fuse_max_seq_len', 'exllama_config', 'modules_to_fuse', 'do_fuse']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "/home/gunwoong/URP2025-1/AutoAWQ/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189b8c8b5eaf43d7a86b7ac7c9ea4b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak PT alloc : 5474.0 MiB\n",
      "=== Meta-Llama-3.1-8B 8B INT4 Bench ===\n",
      "Load time           :   3.6 s\n",
      "Load GPU memory     :  5462.5 MiB  (PyTorch)\n",
      "Load GPU memory     :  5751.0 MiB  (nvidia-smi)\n",
      "Load CPU RSS        :  1057.3 MiB\n",
      "Inference peak GPU  : 13132.0 MiB\n",
      "Inference speed     :  3815.9 tokens/s (+2048)\n",
      "Wikitext-2 PPL      :    6.64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AwqConfig,\n",
    ")\n",
    "\n",
    "# ── Configuration ─────────────────────────────────────────────────────────────\n",
    "MODEL_NAME     = \"Meta-Llama-3.1-8B\"\n",
    "TOKENIZER_PATH = \"/home/gunwoong/URP2025-1/llama3-8b\"\n",
    "QUANT_PATH     = \"/home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-GEMM-INT4\"\n",
    "SEQLEN         = 2048\n",
    "DEVICE         = torch.device(\"cuda:1\")\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def fmt_mib(x: int) -> float:\n",
    "    return x / (1024**2)\n",
    "\n",
    "def get_driver_gpu_used(gpu_index: int = 0) -> float:\n",
    "    out = subprocess.check_output([\n",
    "        \"nvidia-smi\",\n",
    "        \"--query-gpu=memory.used\",\n",
    "        \"--format=csv,noheader,nounits\",\n",
    "        \"-i\", str(gpu_index)\n",
    "    ])\n",
    "    return max(float(x) for x in out.decode().splitlines() if x.strip())\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_ppl_and_speed(model, tokens, seqlen, device):\n",
    "    total_nll, total_toks = 0.0, 0\n",
    "    # make sure all kernels are done before timing\n",
    "    torch.cuda.synchronize(DEVICE)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in range(tokens.size(1) // seqlen):\n",
    "        chunk = tokens[:, i*seqlen:(i+1)*seqlen].to(DEVICE)\n",
    "        out   = model(chunk, labels=chunk)\n",
    "        total_nll += out.loss.item() * chunk.numel()\n",
    "        total_toks += chunk.numel()\n",
    "\n",
    "    # wait for last kernels\n",
    "    torch.cuda.synchronize(DEVICE)\n",
    "    t1 = time.time()\n",
    "    peak_inference = torch.cuda.max_memory_reserved(device)\n",
    "    ppl       = torch.exp(torch.tensor(total_nll/total_toks)).item()\n",
    "    tok_per_s = total_toks / (t1 - t0)\n",
    "    return ppl, tok_per_s, peak_inference\n",
    "\n",
    "def main():\n",
    "    # 1) Prepare tokens\n",
    "    ds       = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    tokenizer= AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)\n",
    "    big_txt  = \"\\n\\n\".join(ds[\"text\"])\n",
    "    ids      = tokenizer.encode(big_txt, add_special_tokens=False)\n",
    "    tokens   = torch.tensor([ids], dtype=torch.long).repeat(2, 1)\n",
    "\n",
    "    # 2) Load & measure alloc stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    t_start = time.time()\n",
    "\n",
    "    awq_cfg = AwqConfig(bits=4, do_fuse=False, fuse_max_seq_len=SEQLEN)\n",
    "    model   = AutoModelForCausalLM.from_pretrained(\n",
    "        QUANT_PATH,\n",
    "        quantization_config=awq_cfg,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map={ \"\": DEVICE.index },\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    torch.cuda.synchronize(DEVICE)\n",
    "    peak = torch.cuda.max_memory_reserved(DEVICE)\n",
    "    print(f\"Peak PT alloc : {peak/1024**2:.1f} MiB\")\n",
    "    load_time    = time.time() - t_start\n",
    "    \n",
    "    # PyTorch allocator usage (optional)\n",
    "    gpu_used_tensor = fmt_mib(torch.cuda.memory_allocated(DEVICE))\n",
    "\n",
    "    # Real driver-reported peak (≈nvidia-smi)\n",
    "    gpu_used_driver = get_driver_gpu_used(1)\n",
    "\n",
    "    # CPU RSS\n",
    "    cpu_rss = fmt_mib(psutil.Process(os.getpid()).memory_info().rss)\n",
    "\n",
    "    # 3) Eval PPL + speed\n",
    "    ppl, speed, inf_peak = eval_ppl_and_speed(model, tokens, SEQLEN, DEVICE)\n",
    "\n",
    "    # 4) Print bench\n",
    "    print(f\"=== {MODEL_NAME} 8B INT4 Bench ===\")\n",
    "    print(f\"Load time           : {load_time:5.1f} s\")\n",
    "    print(f\"Load GPU memory     : {gpu_used_tensor:7.1f} MiB  (PyTorch)\")\n",
    "    print(f\"Load GPU memory     : {gpu_used_driver:7.1f} MiB  (nvidia-smi)\")  # ← your 9793 MiB\n",
    "    print(f\"Load CPU RSS        : {cpu_rss:7.1f} MiB\")\n",
    "    print(f\"Inference peak GPU  : {inf_peak/1024**2:7.1f} MiB\")\n",
    "    print(f\"Inference speed     : {speed:7.1f} tokens/s (+{SEQLEN})\")\n",
    "    print(f\"Wikitext-2 PPL      : {ppl:7.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09943b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d47e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (289076 > 131072). Running this sequence through the model will result in indexing errors\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:04<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Meta-Llama-3.1-8B 8B AWQ-Marlin INT4 Bench ===\n",
      "Load time           :   6.0 s\n",
      "Peak PT alloc       :  5438.0 MiB\n",
      "Load GPU memory     :  5436.9 MiB  (torch)\n",
      "Load GPU memory     :   355.0 MiB  (nvidia-smi)\n",
      "Load CPU RSS        :  1916.3 MiB\n",
      "Inference peak GPU  :  9314.0 MiB\n",
      "Inference speed     :  3944.7 tokens/s (+2048)\n",
      "Wikitext-2 PPL      :    7.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "\n",
    "# ── Configuration ─────────────────────────────────────────────────────────────\n",
    "MODEL_NAME     = \"Meta-Llama-3.1-8B\"\n",
    "TOKENIZER_PATH = \"/home/gunwoong/URP2025-1/llama3-8b\"\n",
    "QUANT_PATH     = \"/home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-Marlin-INT4\"\n",
    "SEQLEN         = 2048\n",
    "DEVICE         = torch.device(\"cuda:0\")\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def fmt_mib(x: int) -> float:\n",
    "    return x / (1024**2)\n",
    "\n",
    "def get_driver_gpu_used(gpu_index: int = 0) -> float:\n",
    "    out = subprocess.check_output([\n",
    "        \"nvidia-smi\",\n",
    "        \"--query-gpu=memory.used\",\n",
    "        \"--format=csv,noheader,nounits\",\n",
    "        \"-i\", str(gpu_index)\n",
    "    ])\n",
    "    return max(float(x) for x in out.decode().splitlines() if x.strip())\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_ppl_and_speed(model, tokens, seqlen, device):\n",
    "    total_nll, total_toks = 0.0, 0\n",
    "    torch.cuda.synchronize(device)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in range(tokens.size(1) // seqlen):\n",
    "        chunk = tokens[:, i*seqlen:(i+1)*seqlen].to(device)\n",
    "        out   = model(chunk, labels=chunk)\n",
    "        total_nll += out.loss.item() * chunk.numel()\n",
    "        total_toks += chunk.numel()\n",
    "\n",
    "    torch.cuda.synchronize(device)\n",
    "    t1 = time.time()\n",
    "    inf_peak = torch.cuda.max_memory_reserved(device)\n",
    "    ppl       = torch.exp(torch.tensor(total_nll/total_toks)).item()\n",
    "    speed     = total_toks / (t1 - t0)\n",
    "    return ppl, speed, inf_peak\n",
    "\n",
    "def main():\n",
    "    # 1) Prepare tokens\n",
    "    ds        = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)\n",
    "    big_txt   = \"\\n\\n\".join(ds[\"text\"])\n",
    "    ids       = tokenizer.encode(big_txt, add_special_tokens=False)\n",
    "    tokens    = torch.tensor([ids], dtype=torch.long)\n",
    "\n",
    "    # 2) Load & measure alloc stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    t_start = time.time()\n",
    "\n",
    "    # → Marlin 전용 로드\n",
    "    model = AutoAWQForCausalLM.from_quantized(\n",
    "        QUANT_PATH,\n",
    "        device_map={\"\": \"cuda:0\"},  # 단일 GPU로 몰아넣기\n",
    "        safetensors=True,\n",
    "        fuse_layers=False,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    torch.cuda.synchronize(DEVICE)\n",
    "    peak_load = torch.cuda.max_memory_reserved(DEVICE)\n",
    "    load_time = time.time() - t_start\n",
    "\n",
    "    gpu_allocator = fmt_mib(torch.cuda.memory_allocated(DEVICE))\n",
    "    gpu_driver    = get_driver_gpu_used(1)\n",
    "    cpu_rss       = fmt_mib(psutil.Process(os.getpid()).memory_info().rss)\n",
    "\n",
    "    # 3) Eval PPL + speed\n",
    "    ppl, speed, inf_peak = eval_ppl_and_speed(model, tokens, SEQLEN, DEVICE)\n",
    "\n",
    "    # 4) Print bench\n",
    "    print(f\"\\n=== {MODEL_NAME} 8B AWQ-Marlin INT4 Bench ===\")\n",
    "    print(f\"Load time           : {load_time:5.1f} s\")\n",
    "    print(f\"Peak PT alloc       : {peak_load/1024**2:7.1f} MiB\")\n",
    "    print(f\"Load GPU memory     : {gpu_allocator:7.1f} MiB  (torch)\")\n",
    "    print(f\"Load GPU memory     : {gpu_driver:7.1f} MiB  (nvidia-smi)\")\n",
    "    print(f\"Load CPU RSS        : {cpu_rss:7.1f} MiB\")\n",
    "    print(f\"Inference peak GPU  : {inf_peak/1024**2:7.1f} MiB\")\n",
    "    print(f\"Inference speed     : {speed:7.1f} tokens/s (+{SEQLEN})\")\n",
    "    print(f\"Wikitext-2 PPL      : {ppl:7.2f}\")\n",
    "    print()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f49522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gunwoong/URP2025-1/AutoAWQ/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (289076 > 131072). Running this sequence through the model will result in indexing errors\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:04<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Meta-Llama-3.1-8B 8B AWQ-Marlin INT4 Bench ===\n",
      "Load time           :   6.7 s\n",
      "Peak PT alloc       :  5438.0 MiB\n",
      "Load GPU memory     :  5436.9 MiB  (torch)\n",
      "Load GPU memory     :    18.0 MiB  (nvidia-smi)\n",
      "Load CPU RSS        :  1102.2 MiB\n",
      "Inference peak GPU  : 13132.0 MiB\n",
      "Inference speed     :  3953.9 tokens/s (+2048)\n",
      "Wikitext-2 PPL      :    6.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "\n",
    "# ── Configuration ─────────────────────────────────────────────────────────────\n",
    "MODEL_NAME     = \"Meta-Llama-3.1-8B\"\n",
    "TOKENIZER_PATH = \"/home/gunwoong/URP2025-1/llama3-8b\"\n",
    "QUANT_PATH     = \"/home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-Marlin-INT4\"\n",
    "SEQLEN         = 2048\n",
    "DEVICE         = torch.device(\"cuda:0\")\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def fmt_mib(x: int) -> float:\n",
    "    return x / (1024**2)\n",
    "\n",
    "def get_driver_gpu_used(gpu_index: int = 0) -> float:\n",
    "    out = subprocess.check_output([\n",
    "        \"nvidia-smi\",\n",
    "        \"--query-gpu=memory.used\",\n",
    "        \"--format=csv,noheader,nounits\",\n",
    "        \"-i\", str(gpu_index)\n",
    "    ])\n",
    "    return max(float(x) for x in out.decode().splitlines() if x.strip())\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_ppl_and_speed(model, tokens, seqlen, device):\n",
    "    total_nll, total_toks = 0.0, 0\n",
    "    torch.cuda.synchronize(device)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in range(tokens.size(1) // seqlen):\n",
    "        chunk = tokens[:, i*seqlen:(i+1)*seqlen].to(device)\n",
    "        out   = model(chunk, labels=chunk)\n",
    "        total_nll += out.loss.item() * chunk.numel()\n",
    "        total_toks += chunk.numel()\n",
    "\n",
    "    torch.cuda.synchronize(device)\n",
    "    t1 = time.time()\n",
    "    inf_peak = torch.cuda.max_memory_reserved(device)\n",
    "    ppl       = torch.exp(torch.tensor(total_nll/total_toks)).item()\n",
    "    speed     = total_toks / (t1 - t0)\n",
    "    return ppl, speed, inf_peak\n",
    "\n",
    "def main():\n",
    "    # 1) Prepare tokens\n",
    "    ds        = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)\n",
    "    big_txt   = \"\\n\\n\".join(ds[\"text\"])\n",
    "    ids       = tokenizer.encode(big_txt, add_special_tokens=False)\n",
    "    tokens    = torch.tensor([ids], dtype=torch.long).repeat(2, 1)\n",
    "\n",
    "    # 2) Load & measure alloc stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    t_start = time.time()\n",
    "\n",
    "    # → Marlin 전용 로드\n",
    "    model = AutoAWQForCausalLM.from_quantized(\n",
    "        QUANT_PATH,\n",
    "        device_map={\"\": \"cuda:0\"},  # 단일 GPU로 몰아넣기\n",
    "        safetensors=True,\n",
    "        fuse_layers=False,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    torch.cuda.synchronize(DEVICE)\n",
    "    peak_load = torch.cuda.max_memory_reserved(DEVICE)\n",
    "    load_time = time.time() - t_start\n",
    "\n",
    "    gpu_allocator = fmt_mib(torch.cuda.memory_allocated(DEVICE))\n",
    "    gpu_driver    = get_driver_gpu_used(1)\n",
    "    cpu_rss       = fmt_mib(psutil.Process(os.getpid()).memory_info().rss)\n",
    "\n",
    "    # 3) Eval PPL + speed\n",
    "    ppl, speed, inf_peak = eval_ppl_and_speed(model, tokens, SEQLEN, DEVICE)\n",
    "\n",
    "    # 4) Print bench\n",
    "    print(f\"\\n=== {MODEL_NAME} 8B AWQ-Marlin INT4 Bench ===\")\n",
    "    print(f\"Load time           : {load_time:5.1f} s\")\n",
    "    print(f\"Peak PT alloc       : {peak_load/1024**2:7.1f} MiB\")\n",
    "    print(f\"Load GPU memory     : {gpu_allocator:7.1f} MiB  (torch)\")\n",
    "    print(f\"Load GPU memory     : {gpu_driver:7.1f} MiB  (nvidia-smi)\")\n",
    "    print(f\"Load CPU RSS        : {cpu_rss:7.1f} MiB\")\n",
    "    print(f\"Inference peak GPU  : {inf_peak/1024**2:7.1f} MiB\")\n",
    "    print(f\"Inference speed     : {speed:7.1f} tokens/s (+{SEQLEN})\")\n",
    "    print(f\"Wikitext-2 PPL      : {ppl:7.2f}\")\n",
    "    print()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0adb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gunwoong/.pyenv/versions/awq-marlin/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (289076 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== meta-llama/Meta-Llama-3.1-8B 8B Base FP16 (no fused-attn) ===\n",
      "Load time           :   5.9 s\n",
      "Peak PT alloc       : 17324.0 MiB\n",
      "Load GPU memory     : 15316.5 MiB  (torch)\n",
      "Load GPU memory     : 17643.0 MiB  (nvidia-smi)\n",
      "Load CPU RSS        :   995.0 MiB\n",
      "Inference peak GPU  : 23340.0 MiB\n",
      "Inference speed     :  4061.3 tokens/s (+2048)\n",
      "Wikitext-2 PPL      :    6.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# ── FLASH ATTENTION 완전 비활성화 ────────────────────────────────────────\n",
    "os.environ[\"TRANSFORMERS_NO_TRITON_FLASH_ATTENTION\"] = \"1\"\n",
    "# 또는\n",
    "os.environ[\"DISABLE_FLASH_ATTENTION\"] = \"1\"\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ── Configuration ─────────────────────────────────────────────────────────\n",
    "MODEL_NAME     = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "TOKENIZER_PATH = \"/home/gunwoong/URP2025-1/llama3-8b\"\n",
    "SEQLEN         = 2048\n",
    "DEVICE         = torch.device(\"cuda:0\")\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def fmt_mib(x: int) -> float:\n",
    "    return x / (1024**2)\n",
    "\n",
    "def get_driver_gpu_used(gpu_index: int = 0) -> float:\n",
    "    out = subprocess.check_output([\n",
    "        \"nvidia-smi\",\n",
    "        \"--query-gpu=memory.used\",\n",
    "        \"--format=csv,noheader,nounits\",\n",
    "        \"-i\", str(gpu_index)\n",
    "    ])\n",
    "    return max(float(x) for x in out.decode().splitlines() if x.strip())\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_ppl_and_speed(model, tokens, seqlen, device):\n",
    "    torch.cuda.synchronize(device); t0 = time.time()\n",
    "    total_nll, total_toks = 0.0, 0\n",
    "    for i in range(tokens.size(1) // seqlen):\n",
    "        chunk = tokens[:, i*seqlen:(i+1)*seqlen].to(device)\n",
    "        out   = model(chunk, labels=chunk)\n",
    "        total_nll += out.loss.item() * chunk.numel()\n",
    "        total_toks += chunk.numel()\n",
    "    torch.cuda.synchronize(device); t1 = time.time()\n",
    "    inf_peak = torch.cuda.max_memory_reserved(device)\n",
    "    ppl       = torch.exp(torch.tensor(total_nll/total_toks)).item()\n",
    "    speed     = total_toks / (t1 - t0)\n",
    "    return ppl, speed, inf_peak\n",
    "\n",
    "def main():\n",
    "    # 토큰 준비\n",
    "    ds        = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)\n",
    "    ids       = tokenizer.encode(\"\\n\\n\".join(ds[\"text\"]), add_special_tokens=False)\n",
    "    tokens    = torch.tensor([ids], dtype=torch.long).repeat(2, 1)\n",
    "\n",
    "    # 모델 로드 및 메모리 측정\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    t0 = time.time()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map={\"\": \"cuda:0\"},\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    torch.cuda.synchronize(DEVICE)\n",
    "    peak_load = torch.cuda.max_memory_reserved(DEVICE)\n",
    "    load_time = time.time() - t0\n",
    "\n",
    "    gpu_alloc = fmt_mib(torch.cuda.memory_allocated(DEVICE))\n",
    "    gpu_drv   = get_driver_gpu_used(0)\n",
    "    cpu_rss   = fmt_mib(psutil.Process(os.getpid()).memory_info().rss)\n",
    "\n",
    "    # PPL, 속도 측정\n",
    "    ppl, speed, inf_peak = eval_ppl_and_speed(model, tokens, SEQLEN, DEVICE)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"\\n=== {MODEL_NAME} 8B Base FP16 (no fused-attn) ===\")\n",
    "    print(f\"Load time           : {load_time:5.1f} s\")\n",
    "    print(f\"Peak PT alloc       : {peak_load/1024**2:7.1f} MiB\")\n",
    "    print(f\"Load GPU memory     : {gpu_alloc:7.1f} MiB  (torch)\")\n",
    "    print(f\"Load GPU memory     : {gpu_drv:7.1f} MiB  (nvidia-smi)\")\n",
    "    print(f\"Load CPU RSS        : {cpu_rss:7.1f} MiB\")\n",
    "    print(f\"Inference peak GPU  : {inf_peak/1024**2:7.1f} MiB\")\n",
    "    print(f\"Inference speed     : {speed:7.1f} tokens/s (+{SEQLEN})\")\n",
    "    print(f\"Wikitext-2 PPL      : {ppl:7.2f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab518e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gunwoong/URP2025-1/AutoAWQ/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:04<00:00,  6.74it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07a1acba2194074b88edae34efcc461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bcd88074d841729d17ea1a167d6a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers...:   0%|          | 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WQLinear_Marlin' object has no attribute 'qzeros'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m tokenizer = AutoTokenizer.from_pretrained(QUANT_PATH, use_fast=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 2) Marlin‐INT4 체크포인트 불러오기\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m model = \u001b[43mAutoAWQForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_quantized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQUANT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuse_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Marlin CUDA 커널 활성화\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43msafetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# safetensors 포맷 사용\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# 자동으로 GPU에 배치\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# CPU 메모리 절약 모드\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m.to(DEVICE)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 3) 간단히 생성 예시\u001b[39;00m\n\u001b[32m     23\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33m안녕하세요, 오늘 기분이 어때?\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/URP2025-1/AutoAWQ/awq/models/auto.py:125\u001b[39m, in \u001b[36mAutoAWQForCausalLM.from_quantized\u001b[39m\u001b[34m(self, quant_path, quant_filename, max_seq_len, trust_remote_code, fuse_layers, use_exllama, use_exllama_v2, use_ipex, batch_size, safetensors, device_map, max_memory, offload_folder, download_kwargs, **config_kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     max_seq_len = config_kwargs[\u001b[33m\"\u001b[39m\u001b[33mmax_new_tokens\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    120\u001b[39m     logging.warning(\n\u001b[32m    121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmax_new_tokens argument is deprecated... gracefully \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    122\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msetting max_seq_len=max_new_tokens.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAWQ_CAUSAL_LM_MODEL_MAP\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_quantized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuse_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuse_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_exllama\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_exllama\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_exllama_v2\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_exllama_v2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_ipex\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_ipex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43msafetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/URP2025-1/AutoAWQ/awq/models/base.py:543\u001b[39m, in \u001b[36mBaseAWQForCausalLM.from_quantized\u001b[39m\u001b[34m(self, model_path, model_type, model_filename, max_seq_len, torch_dtype, trust_remote_code, safetensors, fuse_layers, use_exllama, use_exllama_v2, use_ipex, device_map, max_memory, offload_folder, download_kwargs, **config_kwargs)\u001b[39m\n\u001b[32m    541\u001b[39m         warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mSkipping fusing modules because AWQ extension is not installed.\u001b[39m\u001b[33m\"\u001b[39m + msg)\n\u001b[32m    542\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfuse_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_ipex:\n\u001b[32m    546\u001b[39m     \u001b[38;5;66;03m# repack qweight to match the ipex kernel.\u001b[39;00m\n\u001b[32m    547\u001b[39m     model = ipex_post_init(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/URP2025-1/AutoAWQ/awq/models/llama.py:21\u001b[39m, in \u001b[36mLlamaAWQForCausalLM.fuse_layers\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfuse_layers\u001b[39m(model: OldLlamaForCausalLM):\n\u001b[32m     20\u001b[39m     fuser = LlamaFuser(model)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mfuser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfuse_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/URP2025-1/AutoAWQ/awq/models/llama.py:104\u001b[39m, in \u001b[36mLlamaFuser.fuse_transformer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m tqdm.tqdm(\u001b[38;5;28mself\u001b[39m.model.model.layers, desc=\u001b[33m\"\u001b[39m\u001b[33mFusing layers...\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    103\u001b[39m     device = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(module.state_dict().values())).device\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     qkv = \u001b[43mfuse_qkv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     norm_1 = FasterTransformerRMSNorm(\n\u001b[32m    111\u001b[39m         module.input_layernorm.weight, module.input_layernorm.variance_epsilon\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    113\u001b[39m     norm_2 = FasterTransformerRMSNorm(\n\u001b[32m    114\u001b[39m         module.post_attention_layernorm.weight,\n\u001b[32m    115\u001b[39m         module.post_attention_layernorm.variance_epsilon,\n\u001b[32m    116\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/URP2025-1/AutoAWQ/awq/utils/fused_utils.py:140\u001b[39m, in \u001b[36mfuse_qkv\u001b[39m\u001b[34m(module, q_proj, k_proj, v_proj)\u001b[39m\n\u001b[32m    137\u001b[39m qkv_layer.bias = bias\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m [q_proj, k_proj, v_proj]:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m (layer.qweight, \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mqzeros\u001b[49m, layer.scales)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m qkv_layer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/awq-marlin/lib/python3.11/site-packages/torch/nn/modules/module.py:2052\u001b[39m, in \u001b[36mModule.__delattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2050\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules[name]\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2052\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__delattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'WQLinear_Marlin' object has no attribute 'qzeros'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ── 설정 ───────────────────────────────────────────────────────\n",
    "DEVICE     = torch.device(\"cuda:1\")  # 사용할 GPU\n",
    "QUANT_PATH = \"/home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-Marlin-INT4\"\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(QUANT_PATH, use_fast=False)\n",
    "\n",
    "# 2) Marlin‐INT4 체크포인트 불러오기\n",
    "model = AutoAWQForCausalLM.from_quantized(\n",
    "    quant_path=QUANT_PATH,\n",
    "    fuse_layers=True,        # Marlin CUDA 커널 활성화\n",
    "    safetensors=True,        # safetensors 포맷 사용\n",
    "    device_map=\"auto\",       # 자동으로 GPU에 배치\n",
    "    low_cpu_mem_usage=True,  # CPU 메모리 절약 모드\n",
    ").to(DEVICE)\n",
    "\n",
    "# 3) 간단히 생성 예시\n",
    "prompt = \"안녕하세요, 오늘 기분이 어때?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "# .generate() API 는 transformers 인터페이스와 동일합니다.\n",
    "out = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    ")\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb5fa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gunwoong/URP2025-1/AutoAWQ/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (289076 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown AWQLinearVersion marlin",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWikitext-2 PPL      : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mppl\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m7.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     61\u001b[39m torch.cuda.reset_peak_memory_stats()\n\u001b[32m     62\u001b[39m t_start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m model = \u001b[43mAutoAWQForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mQUANT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMarlin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# ← fused CUDA 커널 활성화\u001b[39;49;00m\n\u001b[32m     72\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m model = model.pack(fuse=\u001b[38;5;28;01mTrue\u001b[39;00m).to(DEVICE)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# sync & measure\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/URP2025-1/AutoAWQ/awq/models/auto.py:83\u001b[39m, in \u001b[36mAutoAWQForCausalLM.from_pretrained\u001b[39m\u001b[34m(self, model_path, torch_dtype, trust_remote_code, safetensors, device_map, download_kwargs, low_cpu_mem_usage, use_cache, **model_init_kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_pretrained\u001b[39m(\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m     **model_init_kwargs,\n\u001b[32m     79\u001b[39m ) -> BaseAWQForCausalLM:\n\u001b[32m     80\u001b[39m     model_type = check_and_get_model_type(\n\u001b[32m     81\u001b[39m         model_path, trust_remote_code, **model_init_kwargs\n\u001b[32m     82\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAWQ_CAUSAL_LM_MODEL_MAP\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_init_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/URP2025-1/AutoAWQ/awq/models/base.py:389\u001b[39m, in \u001b[36mBaseAWQForCausalLM.from_pretrained\u001b[39m\u001b[34m(self, model_path, model_type, torch_dtype, trust_remote_code, safetensors, device_map, download_kwargs, low_cpu_mem_usage, use_cache, **model_init_kwargs)\u001b[39m\n\u001b[32m    386\u001b[39m     model_init_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = use_cache\n\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# If not quantized, must load with AutoModelForCausalLM\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m model = \u001b[43mtarget_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_weights_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_init_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    398\u001b[39m model.eval()\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(\n\u001b[32m    401\u001b[39m     model,\n\u001b[32m    402\u001b[39m     model_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    406\u001b[39m     processor=processor,\n\u001b[32m    407\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/awq-marlin/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/awq-marlin/lib/python3.11/site-packages/transformers/modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/awq-marlin/lib/python3.11/site-packages/transformers/modeling_utils.py:4375\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4373\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pre_quantized \u001b[38;5;129;01mor\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4374\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pre_quantized:\n\u001b[32m-> \u001b[39m\u001b[32m4375\u001b[39m         config.quantization_config = \u001b[43mAutoHfQuantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge_quantization_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\n\u001b[32m   4377\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4378\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4379\u001b[39m         config.quantization_config = quantization_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/awq-marlin/lib/python3.11/site-packages/transformers/quantizers/auto.py:206\u001b[39m, in \u001b[36mAutoHfQuantizer.merge_quantization_configs\u001b[39m\u001b[34m(cls, quantization_config, quantization_config_from_args)\u001b[39m\n\u001b[32m    204\u001b[39m         quantization_config = AutoRoundConfig.from_dict(quantization_config)\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m         quantization_config = \u001b[43mAutoQuantizationConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    209\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    210\u001b[39m         quantization_config, (GPTQConfig, AwqConfig, AutoRoundConfig, FbgemmFp8Config, CompressedTensorsConfig)\n\u001b[32m   (...)\u001b[39m\u001b[32m    213\u001b[39m ):\n\u001b[32m    214\u001b[39m     \u001b[38;5;66;03m# special case for GPTQ / AWQ / FbgemmFp8 config collision\u001b[39;00m\n\u001b[32m    215\u001b[39m     loading_attr_dict = quantization_config_from_args.get_loading_attributes()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/awq-marlin/lib/python3.11/site-packages/transformers/quantizers/auto.py:132\u001b[39m, in \u001b[36mAutoQuantizationConfig.from_dict\u001b[39m\u001b[34m(cls, quantization_config_dict)\u001b[39m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    127\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown quantization type, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - supported types are:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(AUTO_QUANTIZER_MAPPING.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m     )\n\u001b[32m    131\u001b[39m target_cls = AUTO_QUANTIZATION_CONFIG_MAPPING[quant_method]\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization_config_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/awq-marlin/lib/python3.11/site-packages/transformers/utils/quantization_config.py:120\u001b[39m, in \u001b[36mQuantizationConfigMixin.from_dict\u001b[39m\u001b[34m(cls, config_dict, return_unused_kwargs, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_dict\u001b[39m(\u001b[38;5;28mcls\u001b[39m, config_dict, return_unused_kwargs=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs):\n\u001b[32m    105\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m    Instantiates a [`QuantizationConfigMixin`] from a Python dictionary of parameters.\u001b[39;00m\n\u001b[32m    107\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    118\u001b[39m \u001b[33;03m        [`QuantizationConfigMixin`]: The configuration object instantiated from those parameters.\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     to_remove = []\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/awq-marlin/lib/python3.11/site-packages/transformers/utils/quantization_config.py:942\u001b[39m, in \u001b[36mAwqConfig.__init__\u001b[39m\u001b[34m(self, bits, group_size, zero_point, version, backend, do_fuse, fuse_max_seq_len, modules_to_fuse, modules_to_not_convert, exllama_config, **kwargs)\u001b[39m\n\u001b[32m    939\u001b[39m     \u001b[38;5;28mself\u001b[39m.do_fuse = do_fuse\n\u001b[32m    940\u001b[39m \u001b[38;5;28mself\u001b[39m.fuse_max_seq_len = fuse_max_seq_len\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/awq-marlin/lib/python3.11/site-packages/transformers/utils/quantization_config.py:953\u001b[39m, in \u001b[36mAwqConfig.post_init\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.backend \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [AwqBackendPackingMethod.AUTOAWQ, AwqBackendPackingMethod.LLMAWQ]:\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    950\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly supported quantization backends in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAwqBackendPackingMethod.AUTOAWQ\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAwqBackendPackingMethod.LLMAWQ\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - not recognized backend \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.backend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    951\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[38;5;28mself\u001b[39m.version = \u001b[43mAWQLinearVersion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.version \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m    955\u001b[39m     AWQLinearVersion.GEMM,\n\u001b[32m    956\u001b[39m     AWQLinearVersion.GEMV,\n\u001b[32m    957\u001b[39m     AWQLinearVersion.EXLLAMA,\n\u001b[32m    958\u001b[39m     AWQLinearVersion.IPEX,\n\u001b[32m    959\u001b[39m ]:\n\u001b[32m    960\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    961\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly supported versions are in [AWQLinearVersion.GEMM, AWQLinearVersion.GEMV, AWQLinearVersion.EXLLAMA, AWQLinearVersion.IPEX] - not recognized version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    962\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/awq-marlin/lib/python3.11/site-packages/transformers/utils/quantization_config.py:87\u001b[39m, in \u001b[36mAWQLinearVersion.from_str\u001b[39m\u001b[34m(version)\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AWQLinearVersion.IPEX\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown AWQLinearVersion \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Unknown AWQLinearVersion marlin"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────\n",
    "MODEL_NAME     = \"Meta-Llama-3.1-8B (Marlin INT4)\"\n",
    "TOKENIZER_PATH = \"/home/gunwoong/URP2025-1/llama3-8b\"\n",
    "QUANT_PATH     = \"/home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-Marlin-INT4\"\n",
    "SEQLEN         = 2048\n",
    "DEVICE         = torch.device(\"cuda:1\")   # Marlin 모델이 올라간 GPU\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def fmt_mib(x: int) -> float:\n",
    "    return x / (1024**2)\n",
    "\n",
    "def get_driver_gpu_used(gpu_index: int = 1) -> float:\n",
    "    out = subprocess.check_output([\n",
    "        \"nvidia-smi\",\n",
    "        \"--query-gpu=memory.used\",\n",
    "        \"--format=csv,noheader,nounits\",\n",
    "        \"-i\", str(gpu_index)\n",
    "    ])\n",
    "    return float(out.decode().strip())\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_ppl_and_speed(model, tokens, seqlen):\n",
    "    # ensure all queued kernels are done before timing\n",
    "    torch.cuda.synchronize(DEVICE)\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_nll, total_toks = 0.0, 0\n",
    "    for i in range(tokens.size(1) // seqlen):\n",
    "        chunk = tokens[:, i*seqlen:(i+1)*seqlen].to(DEVICE)\n",
    "        out   = model(chunk, labels=chunk)\n",
    "        total_nll += out.loss.item() * chunk.numel()\n",
    "        total_toks += chunk.numel()\n",
    "\n",
    "    # wait for last kernels\n",
    "    torch.cuda.synchronize(DEVICE)\n",
    "    t1 = time.time()\n",
    "\n",
    "    ppl       = torch.exp(torch.tensor(total_nll / total_toks)).item()\n",
    "    tok_per_s = total_toks / (t1 - t0)\n",
    "    return ppl, tok_per_s\n",
    "\n",
    "def main():\n",
    "    # 1) Prepare tokens\n",
    "    ds_test   = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)\n",
    "    big_txt   = \"\\n\\n\".join(ds_test[\"text\"])\n",
    "    ids       = tokenizer.encode(big_txt, add_special_tokens=False)\n",
    "    tokens    = torch.tensor([ids], dtype=torch.long)\n",
    "\n",
    "    # 2) Load quantized Marlin model with fused kernels\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    t_start = time.time()\n",
    "\n",
    "    model = AutoAWQForCausalLM.from_pretrained(\n",
    "        QUANT_PATH,\n",
    "        version=\"Marlin\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_cache=False,        # ← fused CUDA 커널 활성화\n",
    "    )\n",
    "    model = model.pack(fuse=True).to(DEVICE)\n",
    "    # sync & measure\n",
    "    torch.cuda.synchronize(DEVICE)\n",
    "    load_time = time.time() - t_start\n",
    "\n",
    "    # PyTorch allocator memory\n",
    "    gpu_alloc = fmt_mib(torch.cuda.memory_allocated(DEVICE))\n",
    "    # driver-reported memory\n",
    "    gpu_driver = get_driver_gpu_used(DEVICE.index)\n",
    "    cpu_rss = fmt_mib(psutil.Process(os.getpid()).memory_info().rss)\n",
    "\n",
    "    # 3) Eval PPL & Throughput\n",
    "    ppl, speed = eval_ppl_and_speed(model, tokens, SEQLEN)\n",
    "\n",
    "    # 4) Report\n",
    "    print(f\"=== {MODEL_NAME} Bench ===\")\n",
    "    print(f\"Load time           : {load_time:5.1f} s\")\n",
    "    print(f\"Load GPU memory     : {gpu_alloc:7.1f} MiB  (PyTorch)\")\n",
    "    print(f\"Load GPU memory     : {gpu_driver:7.1f} MiB  (nvidia-smi)\")\n",
    "    print(f\"Load CPU RSS        : {cpu_rss:7.1f} MiB\")\n",
    "    print(f\"Inference speed     : {speed:7.1f} tokens/s (+{SEQLEN})\")\n",
    "    print(f\"Wikitext-2 PPL      : {ppl:7.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123c999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=4096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-29 00:49:23 [config.py:793] This model supports multiple tasks: {'generate', 'classify', 'embed', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 05-29 00:49:23 [awq_marlin.py:115] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 05-29 00:49:23 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-29 00:49:23 [core.py:438] Waiting for init message from front-end.\n",
      "INFO 05-29 00:49:23 [core.py:65] Initializing a V1 LLM engine (v0.9.0) with config: model='/home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-GEMM-INT4', speculative_config=None, tokenizer='/home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-GEMM-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-GEMM-INT4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}\n",
      "WARNING 05-29 00:49:23 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ee45788ec10>\n",
      "INFO 05-29 00:49:24 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 05-29 00:49:24 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-29 00:49:24 [gpu_model_runner.py:1531] Starting to load model /home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-GEMM-INT4...\n",
      "INFO 05-29 00:49:24 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-29 00:49:25 [backends.py:35] Using InductorAdaptor\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ad426be4b64b07b437d68048a0c786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-29 00:49:25 [default_loader.py:280] Loading weights took 0.85 seconds\n",
      "INFO 05-29 00:49:26 [gpu_model_runner.py:1549] Model loading took 5.3434 GiB and 1.535618 seconds\n",
      "INFO 05-29 00:49:35 [backends.py:459] Using cache directory: /home/gunwoong/.cache/vllm/torch_compile_cache/26275be213/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-29 00:49:35 [backends.py:469] Dynamo bytecode transform time: 8.59 s\n",
      "INFO 05-29 00:49:38 [backends.py:158] Cache the graph of shape None for later use\n",
      "INFO 05-29 00:50:06 [backends.py:170] Compiling a graph for general shape takes 30.52 s\n",
      "INFO 05-29 00:50:33 [monitor.py:33] torch.compile takes 39.11 s in total\n",
      "INFO 05-29 00:50:33 [kv_cache_utils.py:637] GPU KV cache size: 89,504 tokens\n",
      "INFO 05-29 00:50:33 [kv_cache_utils.py:640] Maximum concurrency for 4,096 tokens per request: 21.85x\n",
      "INFO 05-29 00:51:00 [gpu_model_runner.py:1933] Graph capturing finished in 26 secs, took 0.53 GiB\n",
      "INFO 05-29 00:51:00 [core.py:167] init engine (profile, create kv cache, warmup model) took 93.69 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d60d54b8c74f819cbbb60993eb148d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e042c5798de4ec0ac02059a7b7b7685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 안녕하세요.\n",
      "Generated:  오늘은 대도서관에서 열리길래, 밴드 입단식 봤어요!\n",
      "그리고 금일 2pm 로드 윈\n",
      "Prompt: 대한민국의 수도는?\n",
      "Generated:  Seoul is the capital of Korea.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# 1) 양자화된 모델이 저장된 경로\n",
    "quant_path = \"/home/gunwoong/URP2025-1/Meta-Llama-3.1-8B-AWQ-Marlin-INT4\"\n",
    "\n",
    "# 2) 로드 (device_map=\"auto\" 또는 \"cuda:0\" 등 지정)\n",
    "print(f\"Loading quantized model from: {quant_path}\")\n",
    "model = AutoAWQForCausalLM.from_quantized(\n",
    "    quant_path,\n",
    "    device_map=\"auto\",     # GPU/CPU 자동 분산\n",
    "    safetensors=True       # safetensors 형식이라면 True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    quant_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"Quantized model and tokenizer loaded.\")\n",
    "\n",
    "# 3) (선택) 파이프라인을 이용한 예시 추론\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "prompt = \"Activation-Aware Weight Quantization (AWQ)란 무엇인가?\"\n",
    "outputs = pipe(prompt, max_new_tokens=50, do_sample=True, temperature=0.7)\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fcfe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awq-marlin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
